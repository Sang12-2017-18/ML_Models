{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-01-16T09:29:08.090187Z","iopub.execute_input":"2022-01-16T09:29:08.090551Z","iopub.status.idle":"2022-01-16T09:29:08.127995Z","shell.execute_reply.started":"2022-01-16T09:29:08.090462Z","shell.execute_reply":"2022-01-16T09:29:08.127117Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport math\n\n# Read the CSV file\ndef read_csv_file(filename):\n    df = pd.read_csv(filename)\n    return df\n\n#Get the no. of columns in pandas dataframe\ndef get_no_of_columns(pandas_dataframe):\n    no_of_columns = len(pandas_dataframe.iloc[0]) - 1\n    return no_of_columns\n\ndef get_no_of_training_examples(pandas_dataframe):\n    no_of_rows = len(pandas_dataframe)\n    return no_of_rows\n    \n#Function that splits the dataset into input features and target label\ndef split_dataset_feature_target(pandas_dataframe, no_of_columns):\n    df1 = pandas_dataframe.copy()\n    df1_target = df1['Outcome'].copy()\n    df1_input = df1.iloc[:,:no_of_columns].copy()\n    return df1_input, df1_target\n\n# Function to split the dataset into train, dev, test\n# train_percent, dev_percent and test_percent should be in terms of %. Eg: 60,20,20; or 80,10,10\ndef train_dev_test_split(dataset, train_percent, dev_percent, test_percent):\n    if train_percent + dev_percent + test_percent != 100:\n        raise Exception(\"Illegal split ratio\")\n    no_of_rows = len(dataset)\n    dataset.sample(frac=1).reset_index(drop=True) #shuffling rows in the dataset\n    no_of_train_rows = math.floor(train_percent * no_of_rows / 100)\n    no_of_dev_rows = math.floor(dev_percent * no_of_rows / 100)\n    no_of_test_rows = no_of_rows - (no_of_train_rows + no_of_dev_rows)\n    train_set = dataset[:no_of_train_rows]\n    dev_set = dataset[no_of_train_rows: no_of_train_rows+no_of_dev_rows]\n    test_set = dataset[no_of_train_rows+no_of_dev_rows:]\n    return train_set, dev_set, test_set\n\n#filename='/kaggle/input/diabetes-dataset/diabetes2.csv'\n#df = read_csv_file(filename)\n#print(df.head())\n#print(\"\\nTarget Labels: \",df.Outcome.unique())","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:29.387862Z","iopub.execute_input":"2022-01-16T09:30:29.388631Z","iopub.status.idle":"2022-01-16T09:30:29.399050Z","shell.execute_reply.started":"2022-01-16T09:30:29.388598Z","shell.execute_reply":"2022-01-16T09:30:29.398296Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Normalize the dataset (dataset should contain both input features and target label)\n# The function normalizes only the input feature values.\ndef normalize_dataset(pandas_dataframe, no_of_columns):\n    df_normalize = pandas_dataframe.iloc[:,:no_of_columns].copy()\n    df_normalize = (df_normalize - df_normalize.min())/(df_normalize.max() - df_normalize.min())\n    return df_normalize\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:05.079413Z","iopub.execute_input":"2022-01-16T09:31:05.079700Z","iopub.status.idle":"2022-01-16T09:31:05.084704Z","shell.execute_reply.started":"2022-01-16T09:31:05.079668Z","shell.execute_reply":"2022-01-16T09:31:05.083874Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"#ML Model Definitions\nimport numpy as np\n\ndef sigmoid(x):\n    return 1/(1 + np.exp(-x))\n\ndef initialize_weights_and_bias(matrix_shape): \n    print(\"Inside function\")\n    w = np.random.rand(matrix_shape[0], matrix_shape[1])\n    b = 0.0\n    return w,b\n\ndef calc_output(x,w,b):\n    z = np.dot(w.T,x) + b\n    a = sigmoid(z)\n    return a\n\"\"\"\n    The function below is used to compute the cost of logistic regression. Here, I am adding a very\n    small constant epsilon, to the log terms. This is done to avoid divide by zero.\n\"\"\"\n\ndef cost_function(y,yhat):\n    epsilon = 1e-6\n    m = len(y[0])\n    # print(\"m: \", m)\n    # log1 = np.log(yhat.T + epsilon)\n    # print(\"Log1: \", log1)\n    # log2 = np.log(1 + epsilon -yhat.T)\n    # print(\"Log2: \", log2)\n    # prod1 = np.dot(y,log1)\n    # print(\"Prod1: \", prod1)\n    # prod2 = np.dot((1-y),log2)\n    # print(\"Prod2: \", prod2)\n    # sum1 = np.sum(prod1)\n    # print(\"Sum1: \", sum1)\n    # sum2 = np.sum(prod2)\n    # print(\"Sum2: \", sum2)\n    cost = -(np.sum(np.dot(y, np.log(yhat.T + epsilon))) + np.sum(np.dot((1-y),np.log(1 + epsilon -yhat.T))))/m\n    return cost\n","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:32.295598Z","iopub.execute_input":"2022-01-16T09:30:32.295875Z","iopub.status.idle":"2022-01-16T09:30:32.305479Z","shell.execute_reply.started":"2022-01-16T09:30:32.295845Z","shell.execute_reply":"2022-01-16T09:30:32.304558Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Gradient Descent\nimport numpy as np\n\ndef batch_gradient_descent(epochs, learning_rate, train_x, train_y, dev_x, dev_y, w, b):\n    m = len(train_y[0])\n    # print(\"m: \", m)\n    cost_train = {}\n    cost_dev = {}\n    params = {}\n    for iter in range(epochs):\n        train_yhat = calc_output(train_x, w, b)\n        dev_yhat = calc_output(dev_x, w, b)\n        cost_train[iter] = cost_function(train_y,train_yhat)\n        cost_dev[iter] = cost_function(dev_y, dev_yhat)\n        dZ = train_yhat - train_y\n        dW = (np.dot(train_x, dZ.T))/m\n        # print(\"dW: \", dW)\n        db = (np.sum(dZ))/m\n        # print(\"db: \", db)\n        w = w - learning_rate * dW\n        b = b - learning_rate * db\n        params[iter] = {}\n        params[iter][\"w\"] = w\n        params[iter][\"b\"] = b\n    return params, cost_train, cost_dev\n\n#Function to predict\ndef predict(x, w, b, thresh):\n    yhat = calc_output(x, w, b)\n    yhat = yhat.transpose()\n    predicted_values = [1 if ele>thresh else 0 for ele in yhat]\n    return np.array(predicted_values)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:35.080505Z","iopub.execute_input":"2022-01-16T09:30:35.080996Z","iopub.status.idle":"2022-01-16T09:30:35.091185Z","shell.execute_reply.started":"2022-01-16T09:30:35.080954Z","shell.execute_reply":"2022-01-16T09:30:35.090363Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n\ndef find_confusion_matrix(y_predict, y_actual):\n    fp_list = [1 for itr in y_predict if y_predict[itr]==1 and y_actual[itr]==0]\n    fp = len(fp_list)\n    tn_list = [1 for itr in y_predict if y_predict[itr]==0 and y_actual[itr]==0]\n    tn = len(tn_list)\n    tp_list = [1 for itr in y_predict if y_predict[itr]==1 and y_actual[itr]==1]\n    tp = len(tp_list)\n    fn_list = [1 for itr in y_predict if y_predict[itr]==0 and y_actual[itr]==1]\n    fn = len(fn_list)\n    return tp, tn, fp, fn\n\ndef calculate_model_accuracy(train_input, train_target, dev_input, dev_target, test_input, test_target, w, b, threshold):\n    y_train_predict = predict(train_input, w, b, threshold)\n    y_train_predict = y_train_predict.transpose()\n    # print(y_train_predict[:5])\n    y_train_actual = np.array(train_target.transpose().copy())\n    y_train_actual = y_train_actual.reshape(len(y_train_actual), 1)\n    # print(y_train_actual[:5])\n    y_valid_predict = predict(dev_input, w, b, threshold)\n    y_valid_predict = y_valid_predict.transpose()\n    # print(y_valid_predict.shape)\n    y_valid_actual = np.array(dev_target.transpose().copy())\n    y_valid_actual = y_valid_actual.reshape(len(y_valid_actual), 1)\n    # print(y_valid_actual.shape)\n    y_test_predict = predict(test_input, w, b, threshold)\n    y_test_predict = y_test_predict.transpose()\n    y_test_actual = np.array(test_target.transpose().copy())\n    y_test_actual = y_test_actual.reshape(len(y_test_actual), 1)\n    train_tp, train_tn, train_fp, train_fn = find_confusion_matrix(y_train_predict, y_train_actual)\n    train_accuracy = ((train_tp + train_tn) * 100)/(train_tp + train_tn + train_fp + train_fn)\n    dev_tp, dev_tn, dev_fp, dev_fn = find_confusion_matrix(y_valid_predict, y_valid_actual)\n    dev_accuracy = ((dev_tp + dev_tn) * 100)/(dev_tp + dev_tn + dev_fp + dev_fn)\n    test_tp, test_tn, test_fp, test_fn = find_confusion_matrix(y_test_predict, y_test_actual)\n    test_accuracy = ((test_tp + test_tn) * 100)/(test_tp + test_tn + test_fp + test_fn)\n#     train_accuracy = (100 - np.mean(np.abs(y_train_predict - y_train_actual)) * 100) \n#     dev_accuracy = (100 - np.mean(np.abs(y_valid_predict - y_valid_actual)) * 100)\n#     test_accuracy = (100 - np.mean(np.abs(y_test_predict - y_test_actual)) * 100)\n    train_error = 100 - train_accuracy\n    dev_error = 100 - dev_accuracy\n    test_error = 100 - test_accuracy\n    print(\"Training accuracy: \", train_accuracy)\n    print(\"Dev accuracy: \", dev_accuracy)\n    print(\"Test accuracy: \", test_accuracy, \"\\n\")\n    print(\"Training error: \", train_error)\n    print(\"Dev error: \", dev_error)\n    print(\"Test error: \", test_error)\n    accuracy_tuple = (train_accuracy, train_error, dev_accuracy, dev_error, test_accuracy, test_error)\n    return accuracy_tuple\n\ndef find_epoch_value(cost_train_dict, cost_dev_dict, total_epochs):\n    epoch = 0\n    cost_list = [v for k,v in cost_train_dict.items() if k < (total_epochs//2 + total_epochs//6)]\n    dict1 = {k:v for k,v in cost_train_dict.items() if v == min(cost_list) and k < (total_epochs//2 + total_epochs//6)}\n    epoch_list = list(dict1.keys())\n    epoch = epoch_list[0]\n#     ep_iter_list = cost_train_dict.keys()\n#     ep_diff = 50\n#     prev_cost_diff = 0\n#     cost_diff = 0\n#     for itr in range(ep_diff, (len(ep_iter_list)-ep_diff*2)):\n#         prev_cost_diff = cost_diff\n#         itr_prev = itr - ep_diff\n#         itr_next = itr + ep_diff\n#         cost_diff = abs(cost_train_dict[itr_prev] - cost_train_dict[itr_next])\n#         if cost_diff > prev_cost_diff:\n#             epoch = itr\n#         if cost_train_dict[itr] < cost_train_dict[itr_prev] and cost_train_dict[itr] < cost_train_dict[itr_next]:\n#             epoch_list.append(itr)\n#   if epoch == 0:\n#         dict1 = {k:v for k,v in cost_dev_dict.items() if v == min(cost_dev_dict.values())}\n#         epoch_list = list(dict1.keys())\n#         epoch = epoch_list[0]\n    return epoch\n\ndef find_tpr_fpr(y_valid_predict, y_valid_actual):\n    fp_list = [1 for itr in y_valid_predict if y_valid_predict[itr]==1 and y_valid_actual[itr]==0]\n    fp = len(fp_list)\n    tn_list = [1 for itr in y_valid_predict if y_valid_predict[itr]==0 and y_valid_actual[itr]==0]\n    tn = len(tn_list)\n    tp_list = [1 for itr in y_valid_predict if y_valid_predict[itr]==1 and y_valid_actual[itr]==1]\n    tp = len(tp_list)\n    fn_list = [1 for itr in y_valid_predict if y_valid_predict[itr]==0 and y_valid_actual[itr]==1]\n    fn = len(fn_list)\n    fpr = 0 if (fp+tn)==0 else fp/(fp+tn)\n    tpr = 0 if (tp+fn)==0 else tp/(tp+fn)\n    print(tpr, fpr)\n    return tpr, fpr\n\n\n\ndef plot_roc_curve(dev_input, dev_target, w, b):\n    # print(y_train_actual.shape)\n    y_valid_predict = calc_output(dev_input, w, b)\n    y_valid_predict = y_valid_predict.transpose()\n    y_valid_predict = y_valid_predict.reshape(len(y_valid_predict), 1)\n    # print(y_valid_predict)\n    y_valid_actual = np.array(dev_target.copy())\n    y_valid_actual = y_valid_actual.transpose()\n    y_valid_actual = y_valid_actual.reshape(len(y_valid_actual), 1)\n    # print(y_valid_actual)\n    threshold_list = [0.06,0.08,0.1,0.12,0.14,0.16,0.18,0.2,0.22,0.24,0.26]\n    fpr_list = []\n    tpr_list = []\n    for thresh in threshold_list:\n        predicted_values = np.array([1 if ele>thresh else 0 for ele in y_valid_predict])\n        predicted_values = predicted_values.reshape(len(predicted_values),1)\n        tpr, fpr = find_tpr_fpr(predicted_values, y_valid_actual)\n        fpr_list.append(fpr)\n        tpr_list.append(tpr)\n    # print(y_valid_actual.shape)\n    auc_score = abs(np.trapz(tpr_list,fpr_list))\n    print(\"AUC Score: \", auc_score)\n    plt.plot(fpr_list,tpr_list,c=\"red\")\n    plt.title(\"ROC Curve\")\n    plt.xlabel(\"False Positive Rate\")\n    plt.ylabel(\"True Positive Rate\")\n    plt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:38.551526Z","iopub.execute_input":"2022-01-16T09:30:38.551920Z","iopub.status.idle":"2022-01-16T09:30:38.584163Z","shell.execute_reply.started":"2022-01-16T09:30:38.551892Z","shell.execute_reply":"2022-01-16T09:30:38.583530Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"#The entire ML model put into one function \ndef logistic_regression_model(input_dataset):\n    #Step 1: Normalize the input features\n    no_of_columns = get_no_of_columns(input_dataset)\n    input_dataset_normalized = normalize_dataset(input_dataset, no_of_columns)\n    \n    #Step 2: Split dataset into train, dev, test. Percentage of split is given as input\n    #Training set, dev set and test set percentage should add up to 100.\n    training_set_percentage = 70\n    dev_set_percentage = 20\n    test_set_percentage = 10\n    train_set, dev_set, test_set = train_dev_test_split(input_dataset, training_set_percentage, dev_set_percentage, test_set_percentage)\n    #print(len(train_set))\n    #print(len(dev_set))\n    #print(len(test_set))\n    \n    #Step 3: Split train, dev and test sets respectively into input features and target label\n    no_of_columns = get_no_of_columns(train_set)\n    train_input, train_target = split_dataset_feature_target(train_set, no_of_columns)\n    dev_input, dev_target = split_dataset_feature_target(dev_set, no_of_columns)\n    test_input, test_target = split_dataset_feature_target(test_set, no_of_columns)\n    #print(train_input.head(),\"\\n\")\n    #print(train_target.head())\n    \n    #Step 4: Reshape the dataset so that it can be fed into the ML model\n    train_input = train_input.transpose()\n    dev_input = dev_input.transpose()\n    test_input = test_input.transpose()\n    train_target = train_target.to_numpy()\n    train_target = train_target.reshape(1,len(train_target))\n    dev_target = dev_target.to_numpy()\n    dev_target = dev_target.reshape(1,len(dev_target))\n    test_target = test_target.to_numpy()\n    test_target = test_target.reshape(1,len(test_target))\n    print(train_input.shape)\n    print(dev_input.shape)\n    print(test_input.shape)\n    \n    #Step 5: Feed the training input set to ML model\n    m_shape = (no_of_columns,1)\n    weights, bias = initialize_weights_and_bias(m_shape)\n    yhat = calc_output(train_input,weights,bias)\n    cost = cost_function(train_target, yhat)\n    print(\"Cost: \", cost)\n    epochs = 6000\n    param_dict, cost_train_dict, cost_dev_dict = batch_gradient_descent(epochs,0.0003,train_input, train_target, dev_input, dev_target, weights, bias)\n    \n    #Step 6: Find the appropriate values for parameters\n    import matplotlib.pyplot as plt\n    plt.title(\"Performance of Gradient Descent against Validation set\")\n    plt.plot(cost_train_dict.keys(), cost_train_dict.values(), c=\"red\", label=\"training set\")\n    plt.plot(cost_dev_dict.keys(), cost_dev_dict.values(), c=\"green\", label=\"validation/dev set\")\n    plt.xlabel(\"Epoch number\")\n    plt.ylabel(\"Cost\")\n    plt.legend()\n    plt.show()\n    \n    input_data = {}\n    input_data[\"train_input\"] = train_input\n    input_data[\"train_target\"] = train_target\n    input_data[\"dev_input\"] = dev_input\n    input_data[\"dev_target\"] = dev_target\n    input_data[\"test_input\"] = test_input\n    input_data[\"test_target\"] = test_target\n    return input_data, param_dict\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:30:46.221237Z","iopub.execute_input":"2022-01-16T09:30:46.221506Z","iopub.status.idle":"2022-01-16T09:30:46.237027Z","shell.execute_reply.started":"2022-01-16T09:30:46.221470Z","shell.execute_reply":"2022-01-16T09:30:46.236271Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Dataset preparation\nimport numpy as np\nimport pandas as pd\n\n# Step 1: Read dataset from file\nfilename = '/kaggle/input/diabetes-dataset/diabetes2.csv'\ndf = read_csv_file(filename)\n\ninput_data, param_dict = logistic_regression_model(df)\n\n# Step 2: Split into input features and target label\n# no_of_columns = get_no_of_columns(df)\n# df_input, df_target = split_dataset_feature_target(df, no_of_columns)\n\n# # Normalize input dataset\n# df_normalize = normalize_dataset(df_input, no_of_columns)\n# df_normalize['Outcome'] = df['Outcome']\n\n# #Split the data into Training Set, Dev/Validation Set and Test Set\n# df_normalize.sample(frac=1).reset_index(drop=True)\n# train_set = df_normalize[:461].copy()\n# dev_set = df_normalize[461:614].copy()\n# test_set = df_normalize[614:].copy()\n# print(len(train_set)) # Output: 461\n# print(len(dev_set))  # Output: 153\n# print(len(test_set)) # Output: 154","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:31:11.648171Z","iopub.execute_input":"2022-01-16T09:31:11.648458Z","iopub.status.idle":"2022-01-16T09:31:13.509770Z","shell.execute_reply.started":"2022-01-16T09:31:11.648430Z","shell.execute_reply":"2022-01-16T09:31:13.508918Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"#Step 7: Calculate accuracy of model\n# epoch_value = find_epoch_value(cost_train_dict, cost_dev_dict, epochs)\nepoch_value = 1100\nprint(\"Epoch value: \", epoch_value)\nw = param_dict[epoch_value][\"w\"]\nb = param_dict[epoch_value][\"b\"]\n# accuracy_tuple = calculate_model_accuracy(train_input, train_target, dev_input, dev_target, test_input, test_target, w, b)\ntrain_input = input_data[\"train_input\"]\ntrain_target = input_data[\"train_target\"]\ndev_input = input_data[\"dev_input\"]\ndev_target = input_data[\"dev_target\"]\ntest_input = input_data[\"test_input\"]\ntest_target = input_data[\"test_target\"]\nplot_roc_curve(train_input, train_target, w, b)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:32:01.479122Z","iopub.execute_input":"2022-01-16T09:32:01.479447Z","iopub.status.idle":"2022-01-16T09:32:01.822113Z","shell.execute_reply.started":"2022-01-16T09:32:01.479412Z","shell.execute_reply":"2022-01-16T09:32:01.821421Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"threshold = 0.2233\naccuracy_tuple = calculate_model_accuracy(train_input, train_target, dev_input, dev_target, test_input, test_target, w, b, threshold)","metadata":{"execution":{"iopub.status.busy":"2022-01-16T09:34:14.714609Z","iopub.execute_input":"2022-01-16T09:34:14.715429Z","iopub.status.idle":"2022-01-16T09:34:14.730948Z","shell.execute_reply.started":"2022-01-16T09:34:14.715379Z","shell.execute_reply":"2022-01-16T09:34:14.729825Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_dataset = train_input.transpose().copy()\n# Visualize the dataset\nimport matplotlib.pyplot as plt\nfig = plt.figure()\nxlabel = 'DiabetesPedigreeFunction'\nylabel = 'Age'\nplt.scatter(train_dataset[xlabel], train_dataset[ylabel])\nplt.title('Dataset Plot')\nplt.xlabel(xlabel)\nplt.ylabel(ylabel)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-03T03:58:06.662001Z","iopub.execute_input":"2022-01-03T03:58:06.662315Z","iopub.status.idle":"2022-01-03T03:58:06.844522Z","shell.execute_reply.started":"2022-01-03T03:58:06.662276Z","shell.execute_reply":"2022-01-03T03:58:06.843695Z"},"trusted":true},"execution_count":null,"outputs":[]}]}